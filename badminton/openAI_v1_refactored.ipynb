{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# OpenAI Badminton Analysis v1 - Refactored Version\\n",
    "\\n",
    "This notebook demonstrates batch processing of badminton videos using the refactored analysis system.\\n",
    "The code has been updated to use the Phase 2 refactored components with enhanced error handling and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a87239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\\n",
    "\\n",
    "from openai import OpenAI\\n",
    "import sys\\n",
    "import os\\n",
    "import csv\\n",
    "import json\\n",
    "from pathlib import Path\\n",
    "\\n",
    "# Add the badminton package to the path\\n",
    "sys.path.insert(0, '/Users/chanakyd/work/vdark/badminton')\\n",
    "\\n",
    "# Import legacy prompt modules (these still work with the refactored system)\\n",
    "import badminton.llm_analysis.bd_prompt as bd_prompt\\n",
    "import badminton.llm_analysis.shot_classification_prompt as scp\\n",
    "\\n",
    "# Import the NEW refactored VideoPoseDataset (recommended)\\n",
    "from badminton.data.video_pose_dataset import VideoPoseDataset\\n",
    "\\n",
    "# Import utility functions\\n",
    "from badminton.utilities.coco_keypoints import create_keypoints_dict\\n",
    "\\n",
    "# Import new utility modules for better file handling\\n",
    "from badminton.utils.video_utils import list_video_files, replace_mp4_with_csv\\n",
    "from badminton.utils.pose_utils import validate_pose_file\\n",
    "\\n",
    "# Import configuration for centralized settings\\n",
    "from badminton.config import Config\\n",
    "\\n",
    "# OpenAI client setup\\n",
    "client = OpenAI(\\n",
    "    api_key=\\"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\\"\\n",
    ")\\n",
    "\\n",
    "model = \\"gpt-5.2-pro-2025-12-11\\"\\n",
    "#model=\\"gpt-5-nano\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Enhanced File Discovery\\n",
    "\\n",
    "The refactored system provides better utilities for discovering and validating video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths using centralized configuration\\n",
    "config = Config()\\n",
    "videos_base = \\"/Users/chanakyd/work/badminton/VB_DATA/videos/\\"\\n",
    "poses_base = \\"/Users/chanakyd/work/vdark/badminton/badminton/VB_DATA/poses/\\"\\n",
    "\\n",
    "# Define the files to process (same as original)\\n",
    "files = [\\n",
    "    '05_Drop_Shot/2022-09-01_17-49-36_dataset_set1_058_003682_003721_B_05.mp4',\\n",
    "    '05_Drop_Shot/2022-09-01_17-49-36_dataset_set1_034_002012_002044_B_05.mp4',\\n",
    "    '05_Drop_Shot/2022-09-07_18-07-54_dataset_set1_020_002514_002548_A_05.mp4',\\n",
    "    '05_Drop_Shot/2022-08-31_19-09-07_dataset_set1_030_003604_003634_A_05.mp4',\\n",
    "    '05_Drop_Shot/2022-09-07_19-32-45_dataset_set1_167_014161_014192_A_05.mp4',\\n",
    "    '14_Smash/2022-09-07_19-11-05_dataset_set1_181_016670_016704_A_14.mp4',\\n",
    "    '14_Smash/2022-09-07_19-24-02_dataset_set1_053_005436_005462_A_14.mp4',\\n",
    "    '14_Smash/2022-08-31_19-48-28_dataset_set1_128_010645_010671_B_14.mp4',\\n",
    "    '14_Smash/2022-09-07_17-46-41_dataset_set1_092_006923_006959_B_14.mp4',\\n",
    "]\\n",
    "\\n",
    "print(f\\"Processing {len(files)} video files...\\")\\n",
    "print(f\\"Videos base path: {videos_base}\\")\\n",
    "print(f\\"Poses base path: {poses_base}\\")\\n",
    "\\n",
    "# Validate that base directories exist\\n",
    "if not os.path.exists(videos_base):\\n",
    "    print(f\\"WARNING: Videos base directory not found: {videos_base}\\")\\n",
    "if not os.path.exists(poses_base):\\n",
    "    print(f\\"WARNING: Poses base directory not found: {poses_base}\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-processing-section",
   "metadata": {},
   "source": [
    "## Enhanced Batch Processing\\n",
    "\\n",
    "The refactored version includes better error handling, progress tracking, and result storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\\n",
    "results = []\\n",
    "errors = []\\n",
    "processed_count = 0\\n",
    "total_files = len(files)\\n",
    "\\n",
    "print(f\\"Starting batch processing of {total_files} files...\\\\n\\")\\n",
    "\\n",
    "for i, file in enumerate(files, 1):\\n",
    "    try:\\n",
    "        print(f\\"[{i}/{total_files}] Processing: {file}\\")\\n",
    "        \\n",
    "        # Construct file paths using utility function\\n",
    "        video_file = os.path.join(videos_base, file)\\n",
    "        pose_file = os.path.join(poses_base, replace_mp4_with_csv(file))\\n",
    "        \\n",
    "        # Validate files exist\\n",
    "        if not os.path.exists(video_file):\\n",
    "            raise FileNotFoundError(f\\"Video file not found: {video_file}\\")\\n",
    "        if not os.path.exists(pose_file):\\n",
    "            raise FileNotFoundError(f\\"Pose file not found: {pose_file}\\")\\n",
    "        \\n",
    "        # Validate pose file format (new feature)\\n",
    "        if not validate_pose_file(pose_file):\\n",
    "            raise ValueError(f\\"Invalid pose file format: {pose_file}\\")\\n",
    "        \\n",
    "        # Create VideoPoseDataset using refactored version\\n",
    "        vpd = VideoPoseDataset(poses_path=pose_file, video_path=video_file)\\n",
    "        \\n",
    "        # Generate enhanced shot description\\n",
    "        shot_description = vpd.get_shot_description_for_player(player='green')\\n",
    "        \\n",
    "        # Create prompt using existing templates\\n",
    "        prompt = scp.SC_BASE_PROMPT + scp.SC_INPUT_PROMPT + shot_description\\n",
    "        \\n",
    "        # Make OpenAI API call with error handling\\n",
    "        try:\\n",
    "            response = client.responses.create(\\n",
    "                model=model,\\n",
    "                input=prompt,\\n",
    "                store=True,\\n",
    "            )\\n",
    "            \\n",
    "            # Store result with metadata\\n",
    "            result = {\\n",
    "                'file': file,\\n",
    "                'video_path': video_file,\\n",
    "                'pose_path': pose_file,\\n",
    "                'response': response.output_text,\\n",
    "                'prompt_length': len(prompt),\\n",
    "                'dataset_info': vpd.get_dataset_summary(),\\n",
    "                'processing_order': i\\n",
    "            }\\n",
    "            results.append(result)\\n",
    "            \\n",
    "            print(f\\"✅ Success: {response.output_text[:100]}...\\")\\n",
    "            processed_count += 1\\n",
    "            \\n",
    "        except Exception as api_error:\\n",
    "            error_info = {\\n",
    "                'file': file,\\n",
    "                'error_type': 'API_ERROR',\\n",
    "                'error': str(api_error),\\n",
    "                'processing_order': i\\n",
    "            }\\n",
    "            errors.append(error_info)\\n",
    "            print(f\\"❌ API Error: {str(api_error)[:100]}...\\")\\n",
    "            \\n",
    "    except Exception as e:\\n",
    "        # Handle file processing errors\\n",
    "        error_info = {\\n",
    "            'file': file,\\n",
    "            'error_type': 'PROCESSING_ERROR',\\n",
    "            'error': str(e),\\n",
    "            'processing_order': i\\n",
    "        }\\n",
    "        errors.append(error_info)\\n",
    "        print(f\\"❌ Processing Error: {str(e)[:100]}...\\")\\n",
    "    \\n",
    "    print()  # Add spacing between files\\n",
    "\\n",
    "# Print summary\\n",
    "print(f\\"\\\\n=== BATCH PROCESSING COMPLETE ===\\")\\n",
    "print(f\\"Successfully processed: {processed_count}/{total_files} files\\")\\n",
    "print(f\\"Errors encountered: {len(errors)} files\\")\\n",
    "print(f\\"Success rate: {(processed_count/total_files)*100:.1f}%\\")"
   ]
  }
 ],
  {
   "cell_type": "markdown",
   "id": "results-analysis-section",
   "metadata": {},
   "source": [
    "## Results Analysis\\n",
    "\\n",
    "Analyze the batch processing results with enhanced analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display successful results\\n",
    "if results:\\n",
    "    print(\\"=== SUCCESSFUL RESULTS ===\\")\\n",
    "    for result in results:\\n",
    "        print(f\\"\\\\nFile: {result['file']}\\")\\n",
    "        print(f\\"Dataset: {result['dataset_info']['frame_count']} frames, {result['dataset_info']['duration']:.2f}s\\")\\n",
    "        print(f\\"Response: {result['response'][:200]}...\\")\\n",
    "        \\n",
    "        # Try to parse JSON response for additional analysis\\n",
    "        try:\\n",
    "            response_json = json.loads(result['response'])\\n",
    "            if 'predictions' in response_json:\\n",
    "                for pred in response_json['predictions']:\\n",
    "                    print(f\\"  → Predicted: {pred.get('label', 'Unknown')} (confidence: {pred.get('confidence', 'N/A')})\\")\\n",
    "        except json.JSONDecodeError:\\n",
    "            print(\\"  → Response is not valid JSON\\")\\n",
    "else:\\n",
    "    print(\\"No successful results to display.\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display errors\\n",
    "if errors:\\n",
    "    print(\\"\\\\n=== ERRORS ENCOUNTERED ===\\")\\n",
    "    for error in errors:\\n",
    "        print(f\\"\\\\nFile: {error['file']}\\")\\n",
    "        print(f\\"Error Type: {error['error_type']}\\")\\n",
    "        print(f\\"Error: {error['error'][:200]}...\\")\\n",
    "        \\n",
    "    # Error statistics\\n",
    "    error_types = {}\\n",
    "    for error in errors:\\n",
    "        error_type = error['error_type']\\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\\n",
    "    \\n",
    "    print(f\\"\\\\nError Statistics:\\")\\n",
    "    for error_type, count in error_types.items():\\n",
    "        print(f\\"  {error_type}: {count} occurrences\\")\\n",
    "else:\\n",
    "    print(\\"\\\\n✅ No errors encountered!\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-analytics-section",
   "metadata": {},
   "source": [
    "## Enhanced Analytics\\n",
    "\\n",
    "The refactored system provides additional analytics capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-analytics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze processing statistics\\n",
    "if results:\\n",
    "    print(\\"=== PROCESSING ANALYTICS ===\\")\\n",
    "    \\n",
    "    # Prompt length statistics\\n",
    "    prompt_lengths = [r['prompt_length'] for r in results]\\n",
    "    print(f\\"\\\\nPrompt Length Statistics:\\")\\n",
    "    print(f\\"  Average: {sum(prompt_lengths)/len(prompt_lengths):.0f} characters\\")\\n",
    "    print(f\\"  Min: {min(prompt_lengths)} characters\\")\\n",
    "    print(f\\"  Max: {max(prompt_lengths)} characters\\")\\n",
    "    \\n",
    "    # Dataset statistics\\n",
    "    frame_counts = [r['dataset_info']['frame_count'] for r in results]\\n",
    "    durations = [r['dataset_info']['duration'] for r in results]\\n",
    "    \\n",
    "    print(f\\"\\\\nDataset Statistics:\\")\\n",
    "    print(f\\"  Average frames: {sum(frame_counts)/len(frame_counts):.1f}\\")\\n",
    "    print(f\\"  Average duration: {sum(durations)/len(durations):.2f}s\\")\\n",
    "    print(f\\"  Total frames processed: {sum(frame_counts)}\\")\\n",
    "    print(f\\"  Total duration processed: {sum(durations):.2f}s\\")\\n",
    "    \\n",
    "    # Shot type analysis (if responses contain predictions)\\n",
    "    predicted_labels = []\\n",
    "    for result in results:\\n",
    "        try:\\n",
    "            response_json = json.loads(result['response'])\\n",
    "            if 'predictions' in response_json:\\n",
    "                for pred in response_json['predictions']:\\n",
    "                    if 'label' in pred:\\n",
    "                        predicted_labels.append(pred['label'])\\n",
    "        except json.JSONDecodeError:\\n",
    "            continue\\n",
    "    \\n",
    "    if predicted_labels:\\n",
    "        label_counts = {}\\n",
    "        for label in predicted_labels:\\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\\n",
    "        \\n",
    "        print(f\\"\\\\nPredicted Shot Types:\\")\\n",
    "        for label, count in sorted(label_counts.items()):\\n",
    "            print(f\\"  {label}: {count} predictions\\")\\n",
    "else:\\n",
    "    print(\\"No results available for analytics.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## Export Results\\n",
    "\\n",
    "Save results to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\\n",
    "if results or errors:\\n",
    "    export_data = {\\n",
    "        'processing_summary': {\\n",
    "            'total_files': total_files,\\n",
    "            'successful': processed_count,\\n",
    "            'errors': len(errors),\\n",
    "            'success_rate': (processed_count/total_files)*100,\\n",
    "            'model_used': model\\n",
    "        },\\n",
    "        'results': results,\\n",
    "        'errors': errors\\n",
    "    }\\n",
    "    \\n",
    "    # Save to file\\n",
    "    output_file = 'batch_processing_results_refactored.json'\\n",
    "    with open(output_file, 'w') as f:\\n",
    "        json.dump(export_data, f, indent=2)\\n",
    "    \\n",
    "    print(f\\"Results exported to: {output_file}\\")\\n",
    "    print(f\\"File size: {os.path.getsize(output_file)} bytes\\")\\n",
    "else:\\n",
    "    print(\\"No results to export.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debug-section",
   "metadata": {},
   "source": [
    "## Debug Information\\n",
    "\\n",
    "Display debug information for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debug-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display debug information\\n",
    "print(\\"=== DEBUG INFORMATION ===\\")\\n",
    "print(f\\"Python path: {sys.path[0]}\\")\\n",
    "print(f\\"Current working directory: {os.getcwd()}\\")\\n",
    "print(f\\"OpenAI model: {model}\\")\\n",
    "\\n",
    "# Test a single file for detailed debugging\\n",
    "if files:\\n",
    "    test_file = files[0]\\n",
    "    print(f\\"\\\\nTesting single file: {test_file}\\")\\n",
    "    \\n",
    "    video_file = os.path.join(videos_base, test_file)\\n",
    "    pose_file = os.path.join(poses_base, replace_mp4_with_csv(test_file))\\n",
    "    \\n",
    "    print(f\\"Video file exists: {os.path.exists(video_file)}\\")\\n",
    "    print(f\\"Pose file exists: {os.path.exists(pose_file)}\\")\\n",
    "    \\n",
    "    if os.path.exists(pose_file):\\n",
    "        try:\\n",
    "            vpd = VideoPoseDataset(poses_path=pose_file, video_path=video_file)\\n",
    "            summary = vpd.get_dataset_summary()\\n",
    "            print(f\\"Dataset loaded successfully: {summary}\\")\\n",
    "            \\n",
    "            # Test new features\\n",
    "            shot_analysis = vpd.analyze_shot_pattern(player='green')\\n",
    "            print(f\\"Shot analysis: {shot_analysis['frame_count']} frames analyzed\\")\\n",
    "            \\n",
    "        except Exception as e:\\n",
    "            print(f\\"Error loading dataset: {e}\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-notes",
   "metadata": {},
   "source": [
    "## Migration Notes\\n",
    "\\n",
    "### What's New in the Refactored Version:\\n",
    "\\n",
    "1. **Enhanced Error Handling**: Graceful handling of missing files, API errors, and processing failures\\n",
    "2. **Better File Validation**: Automatic validation of pose file formats and existence checks\\n",
    "3. **Progress Tracking**: Real-time progress updates and success/failure statistics\\n",
    "4. **Result Analytics**: Comprehensive analysis of processing results and predictions\\n",
    "5. **Export Functionality**: Save results to JSON for further analysis\\n",
    "6. **Debug Information**: Detailed debugging output for troubleshooting\\n",
    "7. **Modular Architecture**: Uses the new refactored components internally\\n",
    "\\n",
    "### Backward Compatibility:\\n",
    "- All original functionality is preserved\\n",
    "- Same OpenAI integration patterns\\n",
    "- Same prompt generation methods\\n",
    "- Same data access patterns\\n",
    "\\n",
    "### Performance Improvements:\\n",
    "- Better memory management\\n",
    "- Optimized data loading\\n",
    "- Enhanced error recovery\\n",
    "- Improved resource cleanup\\n",
    "\\n",
    "### New Features Available:\\n",
    "- `vpd.get_dataset_summary()` - Get video metadata\\n",
    "- `vpd.analyze_shot_pattern()` - Analyze shot patterns\\n",
    "- Enhanced utilities for file discovery and validation\\n",
    "- Better batch processing with progress tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}