{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# OpenAI Badminton Analysis - Refactored Version\n",
    "\n",
    "This notebook demonstrates the refactored badminton analysis system using the new modular architecture.\n",
    "The code has been updated to use the Phase 2 refactored components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a87239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Add the badminton package to the path\n",
    "sys.path.insert(0, '/Users/chanakyd/work/vdark/badminton')\n",
    "\n",
    "# Import legacy prompt modules (these still work with the refactored system)\n",
    "import badminton.llm_analysis.bd_prompt as bd_prompt\n",
    "import badminton.llm_analysis.shot_classification_prompt as scp\n",
    "\n",
    "# Import the NEW refactored VideoPoseDataset (recommended)\n",
    "from badminton.data.video_pose_dataset import VideoPoseDataset\n",
    "\n",
    "# Alternative: You can still use the old import (with deprecation warning)\n",
    "# from badminton.utilities.visualization_utilities import VideoPoseDataset\n",
    "\n",
    "# Import utility functions\n",
    "from badminton.utilities.coco_keypoints import create_keypoints_dict\n",
    "\n",
    "# Import new modular components (optional - for advanced usage)\n",
    "from badminton.data.pose_data_loader import PoseDataLoader\n",
    "from badminton.visualization.pose_visualizer import PoseVisualizer\n",
    "from badminton.features.pose_feature_extractor import PoseFeatureExtractor\n",
    "from badminton.analysis.shot_descriptor import ShotDescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup Video and Pose Data\n",
    "\n",
    "The refactored system maintains the same interface, so existing code works without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d004adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "video_file = \"VB_DATA/poses/05_Drop_Shot/2022-09-01_17-49-36_dataset_set1_058_003682_003721_B_05.mp4\"\n",
    "pose_file = \"VB_DATA/poses/05_Drop_Shot/2022-09-01_17-49-36_dataset_set1_058_003682_003721_B_05.csv\"\n",
    "\n",
    "# Create VideoPoseDataset using the refactored version\n",
    "# This now uses the modular architecture internally but maintains the same interface\n",
    "vpd = VideoPoseDataset(poses_path=pose_file, video_path=video_file)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of frames: {len(vpd)}\")\n",
    "print(f\"Video metadata: {vpd.get_dataset_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-section",
   "metadata": {},
   "source": [
    "## Video Annotation\n",
    "\n",
    "The `annotate_video_with_poses` method works exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "video-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotated video - same interface as before\n",
    "vpd.annotate_video_with_poses(\n",
    "    output_path=\"annotated_video_refactored.mp4\", \n",
    "    include_bboxes=False, \n",
    "    players=['green', 'blue']\n",
    ")\n",
    "\n",
    "print(\"Annotated video created: annotated_video_refactored.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-access-section",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "All existing data access patterns continue to work. The refactored system provides the same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access player data - same as before\n",
    "print(\"Player A data (first frame):\")\n",
    "print(vpd.playera[0])  # Still works!\n",
    "\n",
    "print(\"\\nPlayer B data (first frame):\")\n",
    "print(vpd.playerb[0])  # Still works!\n",
    "\n",
    "# New method: Get specific frame data\n",
    "frame_data_a, frame_data_b = vpd.data_loader.get_frame_data(0)\n",
    "print(\"\\nUsing new modular approach:\")\n",
    "print(f\"Frame 0 - Player A bbox: {frame_data_a[0]}\")\n",
    "print(f\"Frame 0 - Player B bbox: {frame_data_b[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-extraction-section",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Poselet extraction works the same as before, but now uses the modular feature extractor internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract poselets - same interface as before\n",
    "poselets_green = vpd.get_poselets_for_player(player='green')\n",
    "poselets_blue = vpd.get_poselets_for_player(player='blue')\n",
    "\n",
    "print(f\"Extracted {len(poselets_green)} frames of poselets for green player\")\n",
    "print(f\"First frame poselets: {poselets_green[0]}\")\n",
    "\n",
    "# New: Use the modular feature extractor directly\n",
    "feature_extractor = PoseFeatureExtractor()\n",
    "player_data = vpd.data_loader.get_player_data('green')\n",
    "poselets_direct = feature_extractor.extract_poselets_for_player(player_data)\n",
    "\n",
    "print(f\"\\nDirect extraction: {len(poselets_direct)} frames\")\n",
    "print(f\"Results match: {poselets_green == poselets_direct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shot-description-section",
   "metadata": {},
   "source": [
    "## Shot Description Generation\n",
    "\n",
    "Shot description generation maintains the same interface but uses the new modular shot descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shot-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shot description - same interface as before\n",
    "shot_description = vpd.get_shot_description_for_player(player='green')\n",
    "\n",
    "print(\"Shot description for green player:\")\n",
    "print(shot_description[:500] + \"...\" if len(shot_description) > 500 else shot_description)\n",
    "\n",
    "# New: Use the modular shot descriptor directly\n",
    "shot_descriptor = ShotDescriptor()\n",
    "poselets = vpd.get_poselets_for_player('green')\n",
    "description_direct = shot_descriptor.generate_shot_description(poselets, poses_path=pose_file)\n",
    "\n",
    "print(f\"\\nDescriptions match: {shot_description == description_direct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## New Analysis Features\n",
    "\n",
    "The refactored system provides additional analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Analyze shot patterns\n",
    "shot_analysis = vpd.analyze_shot_pattern(player='green')\n",
    "print(\"Shot pattern analysis:\")\n",
    "print(f\"Frame count: {shot_analysis['frame_count']}\")\n",
    "print(f\"Most common poselets: {shot_analysis['poselet_summary']['most_common_poselets']}\")\n",
    "\n",
    "# New: Get feature summary\n",
    "feature_summary = vpd.feature_extractor.get_poselet_summary(poselets_green)\n",
    "print(f\"\\nFeature summary: {feature_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-usage-section",
   "metadata": {},
   "source": [
    "## Using Individual Modules\n",
    "\n",
    "The refactored system allows you to use individual components for more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use individual modules for custom workflows\n",
    "\n",
    "# 1. Load data only\n",
    "data_loader = PoseDataLoader(pose_file)\n",
    "print(f\"Loaded {len(data_loader)} frames of pose data\")\n",
    "\n",
    "# 2. Extract features only\n",
    "extractor = PoseFeatureExtractor()\n",
    "green_data = data_loader.get_player_data('green')\n",
    "features = extractor.extract_poselets_for_player(green_data)\n",
    "print(f\"Extracted features for {len(features)} frames\")\n",
    "\n",
    "# 3. Analyze shots only\n",
    "analyzer = ShotDescriptor()\n",
    "analysis = analyzer.analyze_shot_pattern(features)\n",
    "print(f\"Analysis complete: {analysis['frame_count']} frames analyzed\")\n",
    "\n",
    "# 4. Create visualizations only (if you have video frames)\n",
    "visualizer = PoseVisualizer()\n",
    "print(\"Visualizer ready for custom rendering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "openai-integration-section",
   "metadata": {},
   "source": [
    "## OpenAI Integration\n",
    "\n",
    "The OpenAI integration works exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client setup (add your API key)\n",
    "client = OpenAI(\n",
    "    api_key=\"your-api-key-here\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# Generate prompt using the same method as before\n",
    "prompt = scp.SC_BASE_PROMPT + scp.SC_INPUT_PROMPT + vpd.get_shot_description_for_player(player='green')\n",
    "\n",
    "print(\"Prompt generated successfully!\")\n",
    "print(f\"Prompt length: {len(prompt)} characters\")\n",
    "print(\"\\nFirst 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openai-call",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make OpenAI API call (uncomment and add your API key to use)\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ],\n",
    "#     max_tokens=500\n",
    "# )\n",
    "# \n",
    "# print(\"OpenAI Response:\")\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "print(\"OpenAI integration ready - add your API key to test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-notes",
   "metadata": {},
   "source": [
    "## Migration Notes\n",
    "\n",
    "### What Changed:\n",
    "- **Internal Architecture**: The `VideoPoseDataset` now uses modular components internally\n",
    "- **New Import**: Recommended to use `from badminton.data.video_pose_dataset import VideoPoseDataset`\n",
    "- **Additional Features**: New analysis methods and direct access to individual modules\n",
    "\n",
    "### What Stayed the Same:\n",
    "- **All existing methods** work exactly as before\n",
    "- **Same interface** for `get_shot_description_for_player()`, `get_poselets_for_player()`, etc.\n",
    "- **Same data access** via `vpd.playera`, `vpd.playerb`, etc.\n",
    "- **Same OpenAI integration** patterns\n",
    "\n",
    "### Benefits of Refactoring:\n",
    "- **Better maintainability**: Each component has a single responsibility\n",
    "- **Enhanced testability**: Individual modules can be tested separately\n",
    "- **Improved extensibility**: Easy to add new features or modify existing ones\n",
    "- **Code reusability**: Modules can be used independently\n",
    "- **Better documentation**: Each module is well-documented with clear interfaces\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}