{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# OpenAI Badminton Analysis - Batch Processing\n",
    "\n",
    "This notebook demonstrates batch processing of multiple badminton videos for shot classification.\n",
    "It processes entire directories of pose data and generates comprehensive analysis reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a87239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the badminton package to the path\n",
    "sys.path.insert(0, '/Users/chanakyd/work/vdark/badminton')\n",
    "\n",
    "# Import prompt modules\n",
    "import badminton.llm_analysis.bd_prompt as bd_prompt\n",
    "import badminton.llm_analysis.shot_classification_prompt as scp\n",
    "\n",
    "# Import VideoPoseDataset\n",
    "from badminton.data.video_pose_dataset import VideoPoseDataset\n",
    "\n",
    "# Import utility functions\n",
    "from badminton.utilities.coco_keypoints import create_keypoints_dict\n",
    "\n",
    "# Import analysis components\n",
    "from badminton.data.pose_data_loader import PoseDataLoader\n",
    "from badminton.features.pose_feature_extractor import PoseFeatureExtractor\n",
    "from badminton.analysis.shot_descriptor import ShotDescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup OpenAI Client\n",
    "\n",
    "Configure the OpenAI client for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openai-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client setup (add your API key)\n",
    "client = OpenAI(\n",
    "    api_key=\"your-api-key-here\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "model = \"gpt-4\"\n",
    "max_tokens = 500\n",
    "\n",
    "print(f\"OpenAI client configured with model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-config-section",
   "metadata": {},
   "source": [
    "## Batch Processing Configuration\n",
    "\n",
    "Set up directories and parameters for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_root = Path(\"VB_DATA/poses\")\n",
    "output_dir = Path(\"output/batch_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all shot type directories\n",
    "shot_directories = [d for d in data_root.iterdir() if d.is_dir()]\n",
    "print(f\"Found {len(shot_directories)} shot type directories:\")\n",
    "for shot_dir in shot_directories:\n",
    "    csv_files = list(shot_dir.glob(\"*.csv\"))\n",
    "    print(f\"  {shot_dir.name}: {len(csv_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-functions-section",
   "metadata": {},
   "source": [
    "## Processing Functions\n",
    "\n",
    "Define functions for batch processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(csv_path, video_path=None):\n",
    "    \"\"\"Process a single pose file and generate shot description.\"\"\"\n",
    "    try:\n",
    "        # Create VideoPoseDataset\n",
    "        vpd = VideoPoseDataset(poses_path=str(csv_path), video_path=video_path)\n",
    "        \n",
    "        # Generate shot description for green player\n",
    "        shot_description = vpd.get_shot_description_for_player(player='green')\n",
    "        \n",
    "        # Get analysis summary\n",
    "        analysis = vpd.analyze_shot_pattern(player='green')\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'description': shot_description,\n",
    "            'analysis': analysis,\n",
    "            'frame_count': len(vpd)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def classify_with_openai(shot_description, file_info):\n",
    "    \"\"\"Classify shot using OpenAI API.\"\"\"\n",
    "    try:\n",
    "        # Generate prompt\n",
    "        prompt = scp.SC_BASE_PROMPT + scp.SC_INPUT_PROMPT + shot_description\n",
    "        \n",
    "        # Make API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'classification': response.choices[0].message.content,\n",
    "            'tokens_used': response.usage.total_tokens if hasattr(response, 'usage') else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"Processing functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-processing-section",
   "metadata": {},
   "source": [
    "## Batch Processing Execution\n",
    "\n",
    "Process all files and generate classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "all_results = []\n",
    "processing_stats = {\n",
    "    'total_files': 0,\n",
    "    'successful_processing': 0,\n",
    "    'successful_classification': 0,\n",
    "    'total_tokens': 0,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "# Process each shot type directory\n",
    "for shot_dir in tqdm(shot_directories, desc=\"Processing shot types\"):\n",
    "    shot_type = shot_dir.name\n",
    "    csv_files = list(shot_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nProcessing {shot_type}: {len(csv_files)} files\")\n",
    "    \n",
    "    for csv_file in tqdm(csv_files, desc=f\"Files in {shot_type}\", leave=False):\n",
    "        processing_stats['total_files'] += 1\n",
    "        \n",
    "        # Find corresponding video file\n",
    "        video_file = csv_file.with_suffix('.mp4')\n",
    "        video_path = str(video_file) if video_file.exists() else None\n",
    "        \n",
    "        # Process pose data\n",
    "        pose_result = process_single_file(csv_file, video_path)\n",
    "        \n",
    "        if not pose_result['success']:\n",
    "            processing_stats['errors'].append({\n",
    "                'file': str(csv_file),\n",
    "                'stage': 'pose_processing',\n",
    "                'error': pose_result['error']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        processing_stats['successful_processing'] += 1\n",
    "        \n",
    "        # Classify with OpenAI (uncomment to enable API calls)\n",
    "        # classification_result = classify_with_openai(\n",
    "        #     pose_result['description'], \n",
    "        #     {'file': str(csv_file), 'shot_type': shot_type}\n",
    "        # )\n",
    "        \n",
    "        # For demo purposes, create mock classification\n",
    "        classification_result = {\n",
    "            'success': True,\n",
    "            'classification': f'Mock classification for {shot_type}',\n",
    "            'tokens_used': 150\n",
    "        }\n",
    "        \n",
    "        if classification_result['success']:\n",
    "            processing_stats['successful_classification'] += 1\n",
    "            if classification_result['tokens_used']:\n",
    "                processing_stats['total_tokens'] += classification_result['tokens_used']\n",
    "        else:\n",
    "            processing_stats['errors'].append({\n",
    "                'file': str(csv_file),\n",
    "                'stage': 'classification',\n",
    "                'error': classification_result['error']\n",
    "            })\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'file': str(csv_file),\n",
    "            'shot_type': shot_type,\n",
    "            'frame_count': pose_result.get('frame_count', 0),\n",
    "            'pose_processing': pose_result['success'],\n",
    "            'classification_success': classification_result['success'],\n",
    "            'classification': classification_result.get('classification', ''),\n",
    "            'tokens_used': classification_result.get('tokens_used', 0),\n",
    "            'analysis_summary': pose_result.get('analysis', {})\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Add small delay to respect API rate limits\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(f\"\\nBatch processing complete!\")\n",
    "print(f\"Total files processed: {processing_stats['total_files']}\")\n",
    "print(f\"Successful pose processing: {processing_stats['successful_processing']}\")\n",
    "print(f\"Successful classifications: {processing_stats['successful_classification']}\")\n",
    "print(f\"Total tokens used: {processing_stats['total_tokens']}\")\n",
    "print(f\"Errors encountered: {len(processing_stats['errors'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis-section",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze the batch processing results and generate reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by shot type\n",
    "shot_type_stats = {}\n",
    "for result in all_results:\n",
    "    shot_type = result['shot_type']\n",
    "    if shot_type not in shot_type_stats:\n",
    "        shot_type_stats[shot_type] = {\n",
    "            'total_files': 0,\n",
    "            'successful_classifications': 0,\n",
    "            'total_frames': 0,\n",
    "            'total_tokens': 0\n",
    "        }\n",
    "    \n",
    "    stats = shot_type_stats[shot_type]\n",
    "    stats['total_files'] += 1\n",
    "    stats['total_frames'] += result['frame_count']\n",
    "    stats['total_tokens'] += result['tokens_used']\n",
    "    \n",
    "    if result['classification_success']:\n",
    "        stats['successful_classifications'] += 1\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n=== BATCH PROCESSING STATISTICS ===\")\n",
    "print(f\"{'Shot Type':<25} {'Files':<8} {'Success':<8} {'Frames':<8} {'Tokens':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for shot_type, stats in shot_type_stats.items():\n",
    "    success_rate = stats['successful_classifications'] / stats['total_files'] * 100\n",
    "    print(f\"{shot_type:<25} {stats['total_files']:<8} {success_rate:<7.1f}% {stats['total_frames']:<8} {stats['total_tokens']:<8}\")\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_files = sum(stats['total_files'] for stats in shot_type_stats.values())\n",
    "total_success = sum(stats['successful_classifications'] for stats in shot_type_stats.values())\n",
    "overall_success_rate = total_success / total_files * 100 if total_files > 0 else 0\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'TOTAL':<25} {total_files:<8} {overall_success_rate:<7.1f}% {processing_stats['total_tokens']:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results-section",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the batch processing results to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to JSON\n",
    "results_file = output_dir / \"batch_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'processing_stats': processing_stats,\n",
    "        'shot_type_stats': shot_type_stats,\n",
    "        'detailed_results': all_results\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_file = output_dir / \"batch_summary.csv\"\n",
    "with open(summary_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['file', 'shot_type', 'frame_count', 'classification_success', 'tokens_used'])\n",
    "    \n",
    "    for result in all_results:\n",
    "        writer.writerow([\n",
    "            result['file'],\n",
    "            result['shot_type'],\n",
    "            result['frame_count'],\n",
    "            result['classification_success'],\n",
    "            result['tokens_used']\n",
    "        ])\n",
    "\n",
    "# Save error log if there are errors\n",
    "if processing_stats['errors']:\n",
    "    error_file = output_dir / \"errors.json\"\n",
    "    with open(error_file, 'w') as f:\n",
    "        json.dump(processing_stats['errors'], f, indent=2)\n",
    "    print(f\"\\nErrors saved to: {error_file}\")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  Detailed results: {results_file}\")\n",
    "print(f\"  Summary CSV: {summary_file}\")\n",
    "print(f\"\\nBatch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-notes-section",
   "metadata": {},
   "source": [
    "## Usage Notes\n",
    "\n",
    "### Configuration\n",
    "- **API Key**: Add your OpenAI API key in the setup section\n",
    "- **Model**: Configure the model (gpt-4, gpt-3.5-turbo, etc.)\n",
    "- **Rate Limits**: Adjust the delay between API calls as needed\n",
    "\n",
    "### Output Files\n",
    "- **batch_results.json**: Complete results with all processing details\n",
    "- **batch_summary.csv**: Summary table for easy analysis\n",
    "- **errors.json**: Error log for debugging failed processing\n",
    "\n",
    "### Customization\n",
    "- Modify `process_single_file()` to extract different features\n",
    "- Update `classify_with_openai()` to use different prompts\n",
    "- Add additional analysis metrics as needed\n",
    "\n",
    "### Performance\n",
    "- Processing time depends on file count and API response times\n",
    "- Monitor token usage to manage costs\n",
    "- Use progress bars to track processing status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}