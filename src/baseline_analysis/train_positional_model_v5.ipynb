{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# i want to read know the number of files in each subfolder of VB_DATA/extracted_features_blue_20\n",
    "\n",
    "data_path = \"../VB_DATA/extracted_features_positional_green_20_normalized/\"\n",
    "subfolders = [f.name for f in os.scandir(data_path) if f.is_dir()]\n",
    "file_counts = {}\n",
    "\n",
    "for f in subfolders:\n",
    "    file_counts[f] = len(os.listdir(os.path.join(data_path, f)))\n",
    "\n",
    "print (\"File counts in each subfolder:\")\n",
    "for subfolder, count in file_counts.items():\n",
    "    print(f\"{subfolder}: {count}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "TARGET_FRAMES = 20\n",
    "TARGET_SAMPLES = 138\n",
    "\n",
    "# here in case we want to change the number of categories\n",
    "\"\"\"\n",
    "CATAGORIES = ['00_Short_Serve','01_Cross_Court_Flight','02_Lift','03_Tap_Smash','04_Block',\n",
    "              '05_Drop_Shot','06_Push_Shot','07_Transitional_Slice','08_Cut','09_Rush_Shot',\n",
    "              '10_Defensive_Clear','11_Defensive_Drive','12_Clear','13_Long_Serve','14_Smash',\n",
    "              '15_Flat_Shot','16_Rear_Court_Flat_Drive','17_Short_Flat_Shot']\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "00_Short_Serve: 816\n",
    "02_Lift: 738\n",
    "04_Block: 289\n",
    "05_Drop_Shot: 765\n",
    "06_Push_Shot: 351\n",
    "08_Cut: 552\n",
    "12_Clear: 939\n",
    "13_Long_Serve: 987\n",
    "14_Smash: 284\n",
    "15_Flat_Shot: 267\n",
    "\"\"\"\n",
    "CATEGORIES = ['00_Short_Serve','02_Lift','04_Block',\n",
    "              '05_Drop_Shot','06_Push_Shot','08_Cut',\n",
    "              '12_Clear','13_Long_Serve','14_Smash',\n",
    "              '15_Flat_Shot']\n",
    "\n",
    "\n",
    "def load_data(base_dir):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for idx, cat in enumerate(CATEGORIES):\n",
    "        cat_dir = os.path.join(base_dir, cat)\n",
    "        \n",
    "        num_samples = 0\n",
    "        frames = []\n",
    "        ids = []\n",
    "        for fname in os.listdir(cat_dir):\n",
    "            # i want the first 100 files in each category\n",
    "            \n",
    "            if not fname.endswith('.csv'):\n",
    "                continue\n",
    "            df = pd.read_csv(os.path.join(cat_dir, fname))\n",
    "            if 'Frame' in df.columns:\n",
    "                df = df.drop(columns=['Frame'])\n",
    "            print(df)\n",
    "            arr = df.values.tolist()\n",
    "            print(arr)\n",
    "            if len(arr) < TARGET_FRAMES:\n",
    "                continue\n",
    "            \n",
    "            \"\"\"if num_samples == TARGET_SAMPLES:\n",
    "                break\n",
    "            else: \n",
    "                num_samples += 1    \n",
    "            \"\"\" \n",
    "                       \n",
    "            \"\"\"print(arr)\n",
    "            X.append(arr)\n",
    "            y.append(idx)\"\"\"\n",
    "            frames.append(arr)\n",
    "            ids.append(idx)\n",
    "    \n",
    "        # generate TARGET_SAMPLES random numbers from 0 to len(frames)-1\n",
    "        random_indices = np.random.choice(len(frames), TARGET_SAMPLES, replace=False)\n",
    "\n",
    "        for i in random_indices:\n",
    "            X.append(frames[i])\n",
    "            y.append(ids[i])\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    \n",
    "    y = keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "    return X, y, CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a628cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.manifold import MDS\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 1) Load your labels and precomputed distance matrix\n",
    "# ─────────────────────────────────────────────────\n",
    "df = pd.read_csv('../synthetic_data/synthetic_dataset_30deg.csv')\n",
    "labels = df['point_name'].values\n",
    "n = len(labels)\n",
    "\n",
    "# Suppose you’ve already computed these:\n",
    "#   D_geo, D_sum, D_mean  (each is an n×n numpy array)\n",
    "# For this example we’ll use D_geo:\n",
    "# D_geo = np.load('D_geo.npy')\n",
    "\n",
    "# Suppose `labels` is a list of all label strings,\n",
    "# and `coords2`, `coords3` are dicts mapping each label -> (x2,y2) and -> (x3,y3).\n",
    "\n",
    "\n",
    "# load the labels and the coordinates from file\n",
    "df = pd.read_csv('../synthetic_data/synthetic_dataset_30deg.csv')\n",
    "labels = df['point_name'].values\n",
    "coords2 = {row['point_name']: (ast.literal_eval(row['point2'])) for _, row in df.iterrows()}\n",
    "coords3 = {row['point_name']: (ast.literal_eval(row['point3'])) for _, row in df.iterrows()}\n",
    "\n",
    "def euclid(a, b):\n",
    "    return np.hypot(a[0]-b[0], a[1]-b[1])\n",
    "\n",
    "# Preallocate distance matrices\n",
    "n = len(labels)\n",
    "D_geo, D_sum, D_mean = np.zeros((n,n)), np.zeros((n,n)), np.zeros((n,n))\n",
    "\n",
    "for i, j in product(range(n), range(n)):\n",
    "    lab_i, lab_j = labels[i], labels[j]\n",
    "    d2 = euclid(coords2[lab_i], coords2[lab_j])\n",
    "    d3 = euclid(coords3[lab_i], coords3[lab_j])\n",
    "\n",
    "    D_geo[i,j]  = np.sqrt(d2 * d3)\n",
    "    D_sum[i,j]  = d2 + d3\n",
    "    D_mean[i,j] = 0.5 * (d2 + d3)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 2) Turn distances into an embedding via MDS\n",
    "# ─────────────────────────────────────────────────\n",
    "mds = MDS(n_components=4, dissimilarity='precomputed', random_state=42)\n",
    "embeddings = mds.fit_transform(D_mean)  \n",
    "# embeddings.shape == (n, 4)\n",
    "\n",
    "# Create a lookup from code → vector\n",
    "code2emb = {lab: embeddings[i] for i, lab in enumerate(labels)}\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 3) Build your sequence dataset\n",
    "# ─────────────────────────────────────────────────\n",
    "DATA_DIR = \"../VB_DATA/extracted_features_positional_green_20_normalized\"\n",
    "CATEGORIES = ['00_Short_Serve','02_Lift',\n",
    "              '05_Drop_Shot','08_Cut',\n",
    "              '12_Clear','13_Long_Serve','14_Smash',\n",
    "              '15_Flat_Shot']\n",
    "\n",
    "all_seqs, all_labels = [], []\n",
    "for folder in CATEGORIES:\n",
    "    folder_path = f\"{DATA_DIR}/{folder}\"\n",
    "    label = folder.split('_',1)[1]\n",
    "    for fname in sorted(os.listdir(folder_path)):\n",
    "        if not fname.endswith('.csv'): continue\n",
    "        df = pd.read_csv(f\"{folder_path}/{fname}\")\n",
    "        if 'Frame' in df: df = df.drop(columns=['Frame'])\n",
    "        # df.values is shape (frames, points)\n",
    "        seq_codes = df.values.astype(str)\n",
    "        # map each code to its embedding vector\n",
    "        # result shape: (frames, points, emb_dim)\n",
    "        seq_vecs = np.stack([\n",
    "            np.stack([code2emb[c] for c in row], axis=0)\n",
    "            for row in seq_codes\n",
    "        ], axis=0)\n",
    "        # flatten points into features: (frames, points*emb_dim)\n",
    "        T, P, E = seq_vecs.shape\n",
    "        seq_flat = seq_vecs.reshape(T, P*E)\n",
    "        all_seqs.append(seq_flat)\n",
    "        all_labels.append(label)\n",
    "\n",
    "# pad or truncate to fixed length T=20\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(all_seqs, maxlen=20, dtype='float32', padding='post', truncating='post')\n",
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(all_labels)\n",
    "y = np.eye(len(le.classes_))[y]   # one-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298788b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 1) (Re)load & preprocess your data\n",
    "# ─────────────────────────────────────────────────\n",
    "# — load your sequences and labels as before —\n",
    "# For brevity, here we assume X_all (N, T, F) and y_all (one-hot, N, C) are ready.\n",
    "# If not, build them exactly as in the MDS+LSTM example.\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 2) Define a model-building function for Keras Tuner\n",
    "# ─────────────────────────────────────────────────\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Optional masking if you padded with zeros\n",
    "    model.add(Masking(mask_value=0.0, input_shape=X_train.shape[1:]))\n",
    "    \n",
    "    # Tune number of LSTM layers (1–2) and their units (32–256)\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 2)):\n",
    "        return_seq = (i < hp.get('num_lstm_layers') - 1)\n",
    "        units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=256, step=32)\n",
    "        model.add(LSTM(units, return_sequences=return_seq))\n",
    "        # Optionally add dropout after each LSTM\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)))\n",
    "    \n",
    "    # Final Dense for classification\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "    # Tune the learning rate for Adam\n",
    "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    # Add batch_size as a hyperparameter\n",
    "    hp.Choice('batch_size', [16, 32, 64])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 3) Instantiate the tuner\n",
    "# ─────────────────────────────────────────────────\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='kt_dir',\n",
    "    project_name='lstm_sequence_tuning'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 4) Add a callback to stop early when no improvement\n",
    "# ─────────────────────────────────────────────────\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 5) Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', [16, 32, 64])\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 6) Retrieve the best model and hyperparameters\n",
    "# ─────────────────────────────────────────────────\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"  # LSTM layers: {best_hps.get('num_lstm_layers')}\")\n",
    "\n",
    "\n",
    "# Build the best model and train it fully\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=best_hps.get('batch_size'),\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────\n",
    "# 7) Evaluate on hold-out test set (if you held one out)\n",
    "# ─────────────────────────────────────────────────\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a03d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for this analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes, labels=np.arange(len(le.classes_)))\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Print accuracy by class and overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "overall_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(\"\\nClass Accuracies:\")\n",
    "for cls, acc in zip(le.classes_, class_accuracies):\n",
    "    print(f\"{cls}: {acc:.4f}\")\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")        \n",
    "\n",
    "# Accuracy for the top 3 predictions\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "top_k_acc = top_k_accuracy_score(y_true_classes, y_pred, k=3)\n",
    "print(f\"\\nTop-3 Accuracy: {top_k_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad40791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# assume `model` is your trained keras Sequential model\n",
    "# and X_sample is an array of shape (M, T, F), e.g. a small background set\n",
    "# and X_test is your test set of shape (N, T, F)\n",
    "\n",
    "# 1.1 Select a small background set (e.g. 100 examples) for KernelExplainer\n",
    "background = X_train[np.random.choice(len(X_train), 100, replace=False)]\n",
    "\n",
    "# 1.2 Create a wrapper that takes 2D inputs for KernelExplainer\n",
    "def model_predict(x_flat):\n",
    "    # x_flat comes in shape (K, T*F)\n",
    "    x = x_flat.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "    return model.predict(x)\n",
    "\n",
    "# flatten background\n",
    "bg_flat = background.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
    "\n",
    "# 1.3 Build a KernelExplainer\n",
    "explainer = shap.KernelExplainer(model_predict, bg_flat)\n",
    "\n",
    "# 1.4 Pick one sequence to explain (flattened)\n",
    "idx = 0\n",
    "x_to_explain = X_test[idx:idx+1]\n",
    "x_flat = x_to_explain.reshape(1, -1)\n",
    "\n",
    "# 1.5 Compute SHAP values\n",
    "shap_values = explainer.shap_values(x_flat, nsamples=200)\n",
    "\n",
    "# shap_values is a numpy array of shape (1, T, F) for a single sample\n",
    "# so just use shap_values[0] for the first (and only) sample\n",
    "sv = shap_values[0]  # shape (T, F)\n",
    "\n",
    "# 1.6 Sum over time to get overall feature importances:\n",
    "feature_importance = np.sum(sv, axis=0)  # shape (F,)\n",
    "# or sum over features to see which time‐steps were most important:\n",
    "time_importance    = np.sum(sv, axis=1)  # shape (T,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 1) Compute SHAP values for all test samples\n",
    "# —————————————————————————————————————————————\n",
    "# Flatten test set\n",
    "N, T, F = X_test.shape\n",
    "X_flat = X_test.reshape(N, T*F)\n",
    "\n",
    "# Explainer (built once on background)\n",
    "explainer = shap.KernelExplainer(model_predict, bg_flat)\n",
    "\n",
    "# Compute shap_values for each class: returns list of arrays, each (N, T*F)\n",
    "shap_values = explainer.shap_values(X_flat, nsamples=200)\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 2) Average SHAP values across all test samples\n",
    "# —————————————————————————————————————————————\n",
    "# shap_values shape: (N, T, F)\n",
    "mean_shap = shap_values.mean(axis=0)  # shape (T, F)\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 3) Plot a single heatmap for mean SHAP values\n",
    "# —————————————————————————————————————————————\n",
    "feature_names = [f\"feat_{i}\" for i in range(F)]\n",
    "time_steps    = np.arange(T)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(mean_shap, aspect='auto', cmap='seismic', extent=[0, F, T, 0])\n",
    "plt.colorbar(label=\"SHAP value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Time step\")\n",
    "plt.title(\"Avg SHAP contributions over time (all classes)\")\n",
    "plt.yticks(time_steps)\n",
    "plt.xticks(np.arange(F), feature_names, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52210d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n",
    "# the plt is not showing, just prin it\n",
    "print(\"Mean SHAP values shape:\", mean_shap.shape)\n",
    "print(mean_shap)\n",
    "\n",
    "#save mean_shap to a csv file\n",
    "# Ensure the number of columns matches the shape of mean_shap\n",
    "if mean_shap.shape[1] == len(feature_names):\n",
    "\tmean_shap_df = pd.DataFrame(mean_shap, columns=feature_names)\n",
    "else:\n",
    "\tmean_shap_df = pd.DataFrame(mean_shap, columns=[f\"feat_{i}\" for i in range(mean_shap.shape[1])])\n",
    "mean_shap_df.to_csv('mean_shap_values.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 1) Flatten and explain all your test set\n",
    "# —————————————————————————————————————————————\n",
    "N, T, F = X_test.shape\n",
    "X_flat = X_test.reshape(N, T*F)\n",
    "\n",
    "# Use your existing KernelExplainer (built on bg_flat)\n",
    "# and your model_predict wrapper\n",
    "shap_values = explainer.shap_values(X_flat, nsamples=200)  \n",
    "# shap_values is a list of length C, each array (N, T*F)\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 2) Compute per‐time contributions per class\n",
    "# —————————————————————————————————————————————\n",
    "num_classes = len(shap_values)\n",
    "time_contrib = np.zeros((num_classes, X_test.shape[1]))\n",
    "\n",
    "for c in range(num_classes):\n",
    "    N = shap_values[c].shape[0]\n",
    "    TF = shap_values[c].shape[1]\n",
    "    T = X_test.shape[1]\n",
    "    if TF % T != 0 or T == 0:\n",
    "        print(f\"Warning: For class {c}, cannot reshape ({N},{TF}) to ({N},{T},F). Skipping.\")\n",
    "        continue\n",
    "    F = TF // T\n",
    "    if F == 0:\n",
    "        print(f\"Warning: Computed F=0 for class {c}, skipping.\")\n",
    "        continue\n",
    "    sv_c = shap_values[c].reshape(N, T, F)\n",
    "    # sum over features → (N, T)\n",
    "    time_per_sample = sv_c.sum(axis=2)\n",
    "    # average across all samples in class c (you could also filter by true label)\n",
    "    time_contrib[c] = time_per_sample.mean(axis=0)\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 3) Plot temporal profiles\n",
    "# —————————————————————————————————————————————\n",
    "for c in range(num_classes):\n",
    "    plt.figure(figsize=(6,2.5))\n",
    "    plt.plot(range(T), time_contrib[c], marker='o')\n",
    "    plt.title(f\"Mean SHAP Time-Step Contribution — Class {c}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Sum of SHAP Values\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
