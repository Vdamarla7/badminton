{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Positional Model Training - LSTM with MDS Embeddings\n",
    "\n",
    "This notebook trains an LSTM model for badminton shot classification using positional features.\n",
    "It uses Multi-Dimensional Scaling (MDS) to create embeddings from positional data and applies\n",
    "hyperparameter tuning with Keras Tuner for optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration-section",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First, let's explore the structure of our extracted features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Analyze the number of files in each shot type subfolder\n",
    "data_path = \"../VB_DATA/extracted_features_positional_green_20_normalized/\"\n",
    "subfolders = [f.name for f in os.scandir(data_path) if f.is_dir()]\n",
    "file_counts = {}\n",
    "\n",
    "for folder in subfolders:\n",
    "    file_counts[folder] = len(os.listdir(os.path.join(data_path, folder)))\n",
    "\n",
    "print(\"File counts in each shot type subfolder:\")\n",
    "for subfolder, count in file_counts.items():\n",
    "    print(f\"{subfolder}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-section",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the positional data and prepare it for training. We'll use a balanced sampling approach\n",
    "to ensure equal representation across shot types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration\n",
    "TARGET_FRAMES = 20\n",
    "TARGET_SAMPLES = 138  # Number of samples per category for balanced dataset\n",
    "\n",
    "# Selected shot categories based on data availability\n",
    "CATEGORIES = [\n",
    "    '00_Short_Serve', '02_Lift', '04_Block',\n",
    "    '05_Drop_Shot', '06_Push_Shot', '08_Cut',\n",
    "    '12_Clear', '13_Long_Serve', '14_Smash',\n",
    "    '15_Flat_Shot'\n",
    "]\n",
    "\n",
    "def load_data(base_dir):\n",
    "    \"\"\"Load and balance the dataset across all shot categories.\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for idx, category in enumerate(CATEGORIES):\n",
    "        cat_dir = os.path.join(base_dir, category)\n",
    "        \n",
    "        # Collect all valid sequences for this category\n",
    "        frames = []\n",
    "        ids = []\n",
    "        \n",
    "        for fname in os.listdir(cat_dir):\n",
    "            if not fname.endswith('.csv'):\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(os.path.join(cat_dir, fname))\n",
    "            if 'Frame' in df.columns:\n",
    "                df = df.drop(columns=['Frame'])\n",
    "            \n",
    "            sequence = df.values.tolist()\n",
    "            \n",
    "            # Only include sequences with sufficient frames\n",
    "            if len(sequence) >= TARGET_FRAMES:\n",
    "                frames.append(sequence)\n",
    "                ids.append(idx)\n",
    "        \n",
    "        # Randomly sample TARGET_SAMPLES from available sequences\n",
    "        if len(frames) >= TARGET_SAMPLES:\n",
    "            random_indices = np.random.choice(len(frames), TARGET_SAMPLES, replace=False)\n",
    "            for i in random_indices:\n",
    "                X.append(frames[i])\n",
    "                y.append(ids[i])\n",
    "        else:\n",
    "            print(f\"Warning: {category} has only {len(frames)} sequences, less than target {TARGET_SAMPLES}\")\n",
    "            # Use all available sequences\n",
    "            X.extend(frames)\n",
    "            y.extend(ids)\n",
    "    \n",
    "    X = np.stack(X, axis=0)\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "    \n",
    "    return X, y, CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mds-embedding-section",
   "metadata": {},
   "source": [
    "## MDS Embedding Generation\n",
    "\n",
    "Create embeddings using Multi-Dimensional Scaling (MDS) from synthetic positional data.\n",
    "This transforms discrete positional codes into continuous vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a628cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.manifold import MDS\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load synthetic dataset for MDS embedding generation\n",
    "df = pd.read_csv('../synthetic_data/synthetic_dataset_30deg.csv')\n",
    "labels = df['point_name'].values\n",
    "n = len(labels)\n",
    "\n",
    "# Extract 2D and 3D coordinates from the dataset\n",
    "coords2 = {row['point_name']: ast.literal_eval(row['point2']) for _, row in df.iterrows()}\n",
    "coords3 = {row['point_name']: ast.literal_eval(row['point3']) for _, row in df.iterrows()}\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "    return np.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "# Compute distance matrices\n",
    "n = len(labels)\n",
    "D_geo = np.zeros((n, n))\n",
    "D_sum = np.zeros((n, n))\n",
    "D_mean = np.zeros((n, n))\n",
    "\n",
    "for i, j in product(range(n), range(n)):\n",
    "    lab_i, lab_j = labels[i], labels[j]\n",
    "    d2 = euclidean_distance(coords2[lab_i], coords2[lab_j])\n",
    "    d3 = euclidean_distance(coords3[lab_i], coords3[lab_j])\n",
    "\n",
    "    D_geo[i, j] = np.sqrt(d2 * d3)  # Geometric mean\n",
    "    D_sum[i, j] = d2 + d3           # Sum\n",
    "    D_mean[i, j] = 0.5 * (d2 + d3)  # Arithmetic mean\n",
    "\n",
    "# Generate embeddings using MDS\n",
    "mds = MDS(n_components=4, dissimilarity='precomputed', random_state=42)\n",
    "embeddings = mds.fit_transform(D_mean)\n",
    "\n",
    "# Create lookup dictionary from positional code to embedding vector\n",
    "code2emb = {lab: embeddings[i] for i, lab in enumerate(labels)}\n",
    "\n",
    "print(f\"Generated {len(code2emb)} positional embeddings with {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequence-preparation-section",
   "metadata": {},
   "source": [
    "## Sequence Data Preparation\n",
    "\n",
    "Transform the positional sequences into embedding sequences for LSTM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequence-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequence dataset with embeddings\n",
    "DATA_DIR = \"../VB_DATA/extracted_features_positional_green_20_normalized\"\n",
    "CATEGORIES = [\n",
    "    '00_Short_Serve', '02_Lift', '05_Drop_Shot', '08_Cut',\n",
    "    '12_Clear', '13_Long_Serve', '14_Smash', '15_Flat_Shot'\n",
    "]\n",
    "\n",
    "all_seqs, all_labels = [], []\n",
    "\n",
    "for folder in CATEGORIES:\n",
    "    folder_path = f\"{DATA_DIR}/{folder}\"\n",
    "    label = folder.split('_', 1)[1]  # Extract shot name from folder\n",
    "    \n",
    "    for fname in sorted(os.listdir(folder_path)):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        df = pd.read_csv(f\"{folder_path}/{fname}\")\n",
    "        if 'Frame' in df:\n",
    "            df = df.drop(columns=['Frame'])\n",
    "        \n",
    "        # Convert positional codes to embedding vectors\n",
    "        seq_codes = df.values.astype(str)\n",
    "        \n",
    "        # Map each code to its embedding vector: (frames, points, emb_dim)\n",
    "        seq_vecs = np.stack([\n",
    "            np.stack([code2emb[code] for code in row], axis=0)\n",
    "            for row in seq_codes\n",
    "        ], axis=0)\n",
    "        \n",
    "        # Flatten points into features: (frames, points * emb_dim)\n",
    "        T, P, E = seq_vecs.shape\n",
    "        seq_flat = seq_vecs.reshape(T, P * E)\n",
    "        \n",
    "        all_seqs.append(seq_flat)\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Pad or truncate sequences to fixed length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(all_seqs, maxlen=20, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(all_labels)\n",
    "y = np.eye(len(le.classes_))[y]  # One-hot encoding\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Shot classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter-tuning-section",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Keras Tuner\n",
    "\n",
    "Use Keras Tuner to find optimal hyperparameters for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298788b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Split data into train/validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Further split for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"Build LSTM model with hyperparameter tuning.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Masking layer for padded sequences\n",
    "    model.add(Masking(mask_value=0.0, input_shape=X_train.shape[1:]))\n",
    "    \n",
    "    # Tune number of LSTM layers and their units\n",
    "    num_layers = hp.Int('num_lstm_layers', 1, 2)\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=256, step=32)\n",
    "        model.add(LSTM(units, return_sequences=return_sequences))\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        dropout_rate = hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='kt_dir',\n",
    "    project_name='lstm_sequence_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Run hyperparameter search\n",
    "print(\"Starting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(f\"  LSTM layers: {best_hps.get('num_lstm_layers')}\")\n",
    "print(f\"  Learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "print(\"\\nTraining final model...\")\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    callbacks=[stop_early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the trained model with detailed metrics including confusion matrix and class-wise accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a03d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, top_k_accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes, labels=np.arange(len(le.classes_)))\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Class-wise accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "overall_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "\n",
    "print(\"\\nClass Accuracies:\")\n",
    "for cls, acc in zip(le.classes_, class_accuracies):\n",
    "    print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "# Top-k accuracy\n",
    "top_3_acc = top_k_accuracy_score(y_true_classes, y_pred, k=3)\n",
    "print(f\"Top-3 Accuracy: {top_3_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap-analysis-section",
   "metadata": {},
   "source": [
    "## SHAP Analysis for Model Interpretability\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand which features and time steps\n",
    "are most important for the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad40791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Select background set for SHAP analysis\n",
    "background = X_train[np.random.choice(len(X_train), 100, replace=False)]\n",
    "\n",
    "def model_predict(x_flat):\n",
    "    \"\"\"Wrapper function for SHAP that handles flattened inputs.\"\"\"\n",
    "    x = x_flat.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "    return model.predict(x)\n",
    "\n",
    "# Flatten background data\n",
    "bg_flat = background.reshape(-1, X_train.shape[1] * X_train.shape[2])\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.KernelExplainer(model_predict, bg_flat)\n",
    "\n",
    "# Analyze a single test sample\n",
    "sample_idx = 0\n",
    "x_to_explain = X_test[sample_idx:sample_idx + 1]\n",
    "x_flat = x_to_explain.reshape(1, -1)\n",
    "\n",
    "print(\"Computing SHAP values for sample analysis...\")\n",
    "shap_values = explainer.shap_values(x_flat, nsamples=200)\n",
    "\n",
    "# Reshape SHAP values back to (time_steps, features)\n",
    "sv = shap_values[0].reshape(X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Compute importance scores\n",
    "feature_importance = np.sum(sv, axis=0)  # Sum over time steps\n",
    "time_importance = np.sum(sv, axis=1)     # Sum over features\n",
    "\n",
    "print(f\"\\nFeature importance shape: {feature_importance.shape}\")\n",
    "print(f\"Time step importance shape: {time_importance.shape}\")\n",
    "print(f\"Most important time step: {np.argmax(np.abs(time_importance))}\")\n",
    "print(f\"Most important feature: {np.argmax(np.abs(feature_importance))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap-visualization-section",
   "metadata": {},
   "source": [
    "## SHAP Visualization and Analysis\n",
    "\n",
    "Generate visualizations to understand model behavior across all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute SHAP values for all test samples\n",
    "N, T, F = X_test.shape\n",
    "X_flat = X_test.reshape(N, T * F)\n",
    "\n",
    "print(\"Computing SHAP values for all test samples...\")\n",
    "shap_values_all = explainer.shap_values(X_flat, nsamples=200)\n",
    "\n",
    "# Average SHAP values across all test samples\n",
    "mean_shap = np.mean(shap_values_all, axis=0).reshape(T, F)\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [f\"feat_{i}\" for i in range(F)]\n",
    "time_steps = np.arange(T)\n",
    "\n",
    "# Plot heatmap of mean SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(mean_shap, aspect='auto', cmap='RdBu_r', extent=[0, F, T, 0])\n",
    "plt.colorbar(label=\"Mean SHAP Value\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Time Step\")\n",
    "plt.title(\"Average SHAP Contributions Across Time and Features\")\n",
    "plt.yticks(time_steps)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean SHAP values shape: {mean_shap.shape}\")\n",
    "\n",
    "# Save results to CSV\n",
    "mean_shap_df = pd.DataFrame(mean_shap, columns=feature_names)\n",
    "mean_shap_df.to_csv('mean_shap_values.csv', index=False)\n",
    "print(\"SHAP values saved to 'mean_shap_values.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-analysis-section",
   "metadata": {},
   "source": [
    "## Temporal SHAP Analysis by Class\n",
    "\n",
    "Analyze how different time steps contribute to predictions for each shot class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class temporal contributions\n",
    "num_classes = len(le.classes_)\n",
    "time_contrib = np.zeros((num_classes, T))\n",
    "\n",
    "# Process SHAP values for each class\n",
    "for c in range(num_classes):\n",
    "    if isinstance(shap_values_all, list) and len(shap_values_all) > c:\n",
    "        # Multi-class SHAP output\n",
    "        sv_c = shap_values_all[c].reshape(-1, T, F)\n",
    "    else:\n",
    "        # Single output case - use overall SHAP values\n",
    "        sv_c = shap_values_all.reshape(-1, T, F)\n",
    "    \n",
    "    # Sum over features to get time-step contributions\n",
    "    time_per_sample = sv_c.sum(axis=2)\n",
    "    # Average across all samples\n",
    "    time_contrib[c] = time_per_sample.mean(axis=0)\n",
    "\n",
    "# Plot temporal profiles for each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for c in range(min(num_classes, 8)):  # Plot up to 8 classes\n",
    "    axes[c].plot(range(T), time_contrib[c], marker='o', linewidth=2)\n",
    "    axes[c].set_title(f\"Temporal SHAP - {le.classes_[c]}\")\n",
    "    axes[c].set_xlabel(\"Time Step\")\n",
    "    axes[c].set_ylabel(\"SHAP Contribution\")\n",
    "    axes[c].grid(True, alpha=0.3)\n",
    "    axes[c].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for c in range(num_classes, len(axes)):\n",
    "    axes[c].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTemporal Analysis Summary:\")\n",
    "for c in range(num_classes):\n",
    "    peak_time = np.argmax(np.abs(time_contrib[c]))\n",
    "    peak_value = time_contrib[c][peak_time]\n",
    "    print(f\"{le.classes_[c]}: Peak at time step {peak_time} (value: {peak_value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete pipeline for badminton shot classification using:\n",
    "\n",
    "1. **MDS Embeddings**: Transform positional codes into continuous representations\n",
    "2. **LSTM Architecture**: Capture temporal patterns in shot sequences\n",
    "3. **Hyperparameter Tuning**: Optimize model performance automatically\n",
    "4. **SHAP Analysis**: Understand model decisions and feature importance\n",
    "\n",
    "The model achieves competitive accuracy on shot classification while providing\n",
    "interpretable insights into which temporal patterns and features drive predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}